{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from torch.utils.data import DataLoader\r\n",
    "from torchvision import datasets, transforms\r\n",
    "import tensorflow as tf\r\n",
    "import tensorflow_datasets as tfds\r\n",
    "\r\n",
    "import time \r\n",
    "import torch \r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "from tqdm import tqdm\r\n",
    "from torch import Tensor\r\n",
    "from torch.autograd.grad_mode import F\r\n",
    "\r\n",
    "import shutil\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "from typing import Dict, Optional\r\n",
    "\r\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocesssing \r\n",
    "- dataset generation\r\n",
    "- preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pwd\r\n",
    "filepath = 'drive/MyDrive/colab_sources/cnn_wavelet/8.zip'\r\n",
    "shutil.unpack_archive(filepath, 'dataset')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "def load_data(filepath):\r\n",
    "    data = np.load(filepath)\r\n",
    "    signal = data[\"x\"]\r\n",
    "    label = data[\"y\"]\r\n",
    "    return signal, label"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_dir = \"dataset/8/\"\r\n",
    "split_num = 6\r\n",
    "\r\n",
    "filelist = os.listdir(dataset_dir)\r\n",
    "filelist = sorted(filelist, key=lambda x: int(os.path.splitext(os.path.basename(x))[0][split_num:]))\r\n",
    "\r\n",
    "dataset_x = []\r\n",
    "dataset_x_emp = []\r\n",
    "dataset_y = []\r\n",
    "dataset_y_emp = []\r\n",
    "for i, file in tqdm(enumerate(filelist)):\r\n",
    "    data_x, data_y = load_data(dataset_dir + file)\r\n",
    "\r\n",
    "    if 1 in data_y:\r\n",
    "      # print(f\"data[{i}] contains peak!\")\r\n",
    "      dataset_x.append([data_x])\r\n",
    "      dataset_y.append(data_y)\r\n",
    "    else:\r\n",
    "      # print(f\"data[{i}] doesn't contain peak!\")\r\n",
    "      dataset_x_emp.append([data_x])\r\n",
    "      dataset_y_emp.append(data_y)\r\n",
    "\r\n",
    "def preprocessing(x, y):\r\n",
    "  x = tf.cast(x, tf.float32)*256 / 1000\r\n",
    "  return x, y\r\n",
    "\r\n",
    "dataset_x = np.array(dataset_x[:3201])\r\n",
    "dataset_y = np.array(dataset_y[:3201])\r\n",
    "\r\n",
    "x_train, x_val, y_train, y_val = train_test_split(dataset_x, dataset_y) \r\n",
    "\r\n",
    "train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(2).batch(64).map(preprocessing).prefetch(1)\r\n",
    "val_set = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(2).batch(64).map(preprocessing).prefetch(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(dataset_x[100][0])\r\n",
    "plt.plot(dataset_x[101][0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the custom loss func and model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "'''\r\n",
    "# IoU soft\r\n",
    "note: \"hard\" probabilistic IoU calculation is not differentiable, \r\n",
    "so be sure to use soft probabilistic loss func\r\n",
    "'''\r\n",
    "class IoU_soft(nn.Module):\r\n",
    "  def __init__(self, thresh:float=0.5, smooth:float = 0.0, alpha: float = 0.5):\r\n",
    "    super().__init__()\r\n",
    "    self.thresh = thresh\r\n",
    "    self.smooth = smooth\r\n",
    "    self.alpha = alpha\r\n",
    "\r\n",
    "  def forward(self, inputs: torch.Tensor, targets:torch.Tensor, weights: Optional[torch.Tensor] = None) -> torch.Tensor:\r\n",
    "    '''\r\n",
    "    - alpha: a parameter that sharpen the thresholding.\r\n",
    "      if alpha = 1 -> thresholded input is the same as raw input.\r\n",
    "    '''\r\n",
    "    alpha = self.alpha\r\n",
    "    threshold = self.thresh\r\n",
    "    smooth = self.smooth\r\n",
    "\r\n",
    "    inputs = inputs**alpha / (inputs**alpha + (1 - inputs)**alpha)\r\n",
    "    # inputs = torch.where(thresholded_inputs < threshold, 0, 1)\r\n",
    "\r\n",
    "    # thresholded_inputs = torch.where(thresholded_inputs < threshold, 0, 1)\r\n",
    "    # inputs = (inputs + thresholded_inputs) - inputs.detach()\r\n",
    "\r\n",
    "    batch_size = inputs.shape[0]\r\n",
    "\r\n",
    "    # instead of hard prob calc: intersect = torch.logical_and(outputs, labels)\r\n",
    "    intersect_tensor = (inputs * targets).view(batch_size, -1)\r\n",
    "    intersect = intersect_tensor.sum(-1)\r\n",
    "\r\n",
    "    # insetad of using the hard prob: union = torch.logical_or(outputs, labels)   \r\n",
    "    union_tensor = torch.max(inputs, targets).view(batch_size, -1)\r\n",
    "    union = union_tensor.sum(-1)\r\n",
    "\r\n",
    "    iou = (intersect + smooth) / (union + smooth)  # We smooth our devision to avoid 0/0\r\n",
    "    iou_score = iou.mean()\r\n",
    "\r\n",
    "    return 1 - iou_score\r\n",
    "\r\n",
    "class MLC(nn.Module):\r\n",
    "  def __init__(self, kn_size:int, init_kn: Tensor=None):\r\n",
    "    super().__init__()\r\n",
    "    self.init_kn = init_kn\r\n",
    "    if kn_size % 2 == 1:\r\n",
    "      padding_num = int((kn_size-1)/2)\r\n",
    "      print(kn_size, padding_num)\r\n",
    "    else:\r\n",
    "      kn_size = kn_size + 1\r\n",
    "      padding_num = int((kn_size-1)/2)\r\n",
    "      print(kn_size, padding_num)\r\n",
    "    self.conv1 = nn.Conv1d(1, 1, kernel_size=kn_size, stride=1, padding=padding_num)\r\n",
    "    if init_kn is not None:\r\n",
    "      self.conv1.weight = nn.Parameter(init_kn.expand(1,1,-1).contiguous())\r\n",
    "    self.net = nn.Sequential(\r\n",
    "        self.conv1,\r\n",
    "        # nn.Softmax(2)\r\n",
    "        # as of now, it would increase the prediction accuracy to get rid of the softmax layer.\r\n",
    "    )\r\n",
    "  def forward(self, x):\r\n",
    "    return self.net(x)\r\n",
    "\r\n",
    "def kn_initializer(length: int,sigma: float =1.0, amp: float =1.0):\r\n",
    "  if length % 2 == 0:\r\n",
    "    length += 1\r\n",
    "  x = np.arange(-length/2 + 1/2, length/2 + 1/2, 1)\r\n",
    "  mother= np.exp(-np.power(x,2)/np.power(sigma, 2))\r\n",
    "  mother = amp * mother\r\n",
    "  return torch.Tensor(mother)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Learning and evaluation of the learned conv kernel"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kn_size = 64\r\n",
    "epochs = 1000\r\n",
    "\r\n",
    "init_kn = kn_initializer(kn_size, sigma=10, amp=0.1)\r\n",
    "model = MLC(kn_size = kn_size, init_kn=init_kn)\r\n",
    "opt = optim.Adadelta(model.parameters(), lr=1e-1)\r\n",
    "\r\n",
    "init_kernel = model.state_dict()[\"net.0.weight\"][0][0].detach().numpy()\r\n",
    "plt.plot(init_kernel)\r\n",
    "\r\n",
    "model.to(\"cuda\")\r\n",
    "criterion = IoU_soft(alpha=1, smooth=0.1)\r\n",
    "train_loss_arr = []\r\n",
    "\r\n",
    "for e in range(epochs):\r\n",
    "  model.train()\r\n",
    "  train_loss, val_loss, acc = 0., 0., 0.,\r\n",
    "  for x, y in train_set.as_numpy_iterator():\r\n",
    "    opt.zero_grad()\r\n",
    "    x = torch.from_numpy(x)\r\n",
    "    y = torch.from_numpy(y)\r\n",
    "    x = x.to(\"cuda\")\r\n",
    "    y = y.to(\"cuda\")\r\n",
    "    model.train()\r\n",
    "    y_pred = model(x)\r\n",
    "    loss = criterion(y_pred, y)\r\n",
    "    loss.backward()\r\n",
    "    opt.step()\r\n",
    "    train_loss += loss.item()\r\n",
    "  train_loss /= len(train_set)\r\n",
    "  train_loss_arr.append(train_loss)\r\n",
    "\r\n",
    "  if e % 100 == 0:\r\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(16, 4))\r\n",
    "    axs[0].plot(x.to(\"cpu\")[0][0])\r\n",
    "    axs[0].set_title(\"spectra\")\r\n",
    "    axs[1].plot(y_pred.to(\"cpu\")[0][0].detach().numpy())\r\n",
    "    axs[1].set_title(\"y pred\")\r\n",
    "    y_pred = y_pred**criterion.alpha/(y_pred**criterion.alpha + (1-y_pred)**criterion.alpha)\r\n",
    "    axs[1].plot(y_pred.to(\"cpu\")[0][0].detach().numpy())\r\n",
    "    axs[2].plot(y.to(\"cpu\")[0])\r\n",
    "    axs[2].set_title(\"y true\")\r\n",
    "    axs[3].plot(model.state_dict()[\"net.0.weight\"][0][0].to(\"cpu\").numpy())\r\n",
    "    axs[3].set_title(\"kernel1\")\r\n",
    "    # axs[4].plot(model.state_dict()[\"2.weight\"][0][0].to(\"cpu\").numpy())\r\n",
    "    # axs[4].set_title(\"kernel2\")\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "    print(f\"epoch {e}:  train loss: {train_loss}\")\r\n",
    "\r\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12,4))\r\n",
    "axs[0].plot(train_loss_arr)\r\n",
    "axs[0].set_title(\"Train loss\")\r\n",
    "axs[1].plot(init_kernel, label=\"init kernel\")\r\n",
    "axs[1].plot(model.state_dict()[\"net.0.weight\"][0][0].to(\"cpu\").numpy(), label=\"trained kernel\")\r\n",
    "axs[1].set_title(\"Initial kernel& Trained kernel\")\r\n",
    "axs[1].legend()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## nn.Softmax(dim=?)\r\n",
    "\r\n",
    "if you want to apply softmax calculation on each input data(spectra), then dim=-1 or 2 is good!\r\n",
    "(this holds true towards LogSoftmax too)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "test_tensor = torch.tensor([[[1.0,0,0, -1]],\r\n",
    "                            [[1,0,0, -1]],\r\n",
    "                            [[-1,-1,-1, -1]],\r\n",
    "                            [[1,0,0, -1]],\r\n",
    "                            [[2,0,0, -1]],\r\n",
    "                            ])\r\n",
    "\r\n",
    "#test_tensor shape: 5, 1, 4\r\n",
    "m = nn.LogSoftmax(dim=-1)\r\n",
    "test_tensor = m(test_tensor)\r\n",
    "print(test_tensor)\r\n",
    "print(test_tensor.shape)\r\n",
    "\r\n",
    "# if you want to apply softmax calculation on each input data(spectra), then dim=-1 or 2 is good!"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[-0.6265, -1.6265, -1.6265, -2.6265]],\n",
      "\n",
      "        [[-0.6265, -1.6265, -1.6265, -2.6265]],\n",
      "\n",
      "        [[-1.3863, -1.3863, -1.3863, -1.3863]],\n",
      "\n",
      "        [[-0.6265, -1.6265, -1.6265, -2.6265]],\n",
      "\n",
      "        [[-0.2780, -2.2780, -2.2780, -3.2780]]])\n",
      "torch.Size([5, 1, 4])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "1baaa464323961626460a32faa3ef133cc86370a46c265138532a3ab2422d515"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}